{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdf43291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "import pandas\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "import attacut\n",
    "import pythainlp\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from IPython.display import display\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5678844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bbbea6bb84846e186e3e0903cd6bc98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/527 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natou\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\natou\\.cache\\huggingface\\hub\\models--clicknext--phayathaibert. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772b35252df74d898b0780735cdc0ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.26M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca84512f06ba41cb82158581efd5b3db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b565df68274465a6f513a18bc4588d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066e2167631641fe8a9a79ceadb1928d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/364 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"clicknext/phayathaibert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenizer_dataset(dataset):\n",
    "    encoded = tokenizer(\n",
    "        dataset['sentence'],\n",
    "        padding='max_length',\n",
    "        max_length=256,#128\n",
    "        truncation=True\n",
    "    )\n",
    "    encoded['labels'] = dataset['type_list']\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b2d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"../model/bert_multiclass_new (1).ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c44a5062",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7d063d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m inputs = tokenizer(\u001b[43mtext\u001b[49m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, padding=\u001b[33m\"\u001b[39m\u001b[33mmax_length\u001b[39m\u001b[33m\"\u001b[39m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, max_length=\u001b[32m256\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "regenerated_sentences = {\n",
    "    \"Paper Work\": \"Buried under mountains of redundant forms, I had a eureka moment ‚Äî what if bureaucracy became our next viral caption trend?\",\n",
    "    \n",
    "    \"Summary\": \"I'll lawyer this beast of a legal doc into a few punchy bullet points, powered by cold logic and a strong cup of coffee.\",\n",
    "    \n",
    "    \"Paraphrase\": \"Translate that techy jargon into robo-speak ‚Äî imagine a toaster explaining its user manual with flair and existential dread.\",\n",
    "    \n",
    "    \"Simulation Role Play as a Character\": \"In character mode, I accidentally unearthed a meme-worthy origin story ‚Äî the kind that breaks the fourth wall with style.\",\n",
    "    \n",
    "    \"Reasoning\": \"Solving the marketing puzzle with deductive finesse ‚Äî the more niche our audience, the sharper our conversion rate.\",\n",
    "    \n",
    "    \"Trending\": \"This trend is so flaming hot, it practically demands a sarcastic remix that'll set the internet on fire.\",\n",
    "    \n",
    "    \"Caption Generation\": \"Let‚Äôs turn the video‚Äôs summary into a scroll-stopping caption that‚Äôs snappy, witty, and dangerously shareable.\",\n",
    "    \n",
    "    \"General\": \"When you're drowning in paperwork, just logic-puzzle your way out ‚Äî bonus points if you do it with sarcasm.\",\n",
    "    \n",
    "    \"Marketing Planning\": \"Our secret sauce? Tapping into what‚Äôs trending and spinning it into content gold ‚Äî like forecasting memes with a crystal ball.\"\n",
    "}\n",
    "# Loop through regenerated sentences\n",
    "for ans, text in regenerated_sentences.items():\n",
    "    print(f\"üìù Text: {text}\")\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=256)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Run model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=-1)\n",
    "        pred = torch.argmax(probs, dim=-1).item()\n",
    "\n",
    "    # Print predicted label and the correct answer tag\n",
    "    print(f\"üîç Predicted label: {ind2type[pred]}\")\n",
    "    print(f\"‚úÖ Expected category: {ans}\")\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3103c321",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
